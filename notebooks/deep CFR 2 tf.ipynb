{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implements Deep CFR Algorithm.\n",
    "\n",
    "See https://arxiv.org/abs/1811.00164.\n",
    "\n",
    "The algorithm defines an `advantage` and `strategy` networks that compute\n",
    "advantages used to do regret matching across information sets and to approximate\n",
    "the strategy profiles of the game. To train these networks a reservoir buffer\n",
    "(other data structures may be used) memory is used to accumulate samples to\n",
    "train the networks.\n",
    "\n",
    "This implementation uses skip connections as described in the paper if two\n",
    "consecutive layers of the advantage or policy network have the same number\n",
    "of units, except for the last connection. Before the last hidden layer\n",
    "a layer normalization is applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DeepCFRTest'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6cc242f8ca9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopen_spiel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepCFRTest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DeepCFRTest'"
     ]
    }
   ],
   "source": [
    "from open_spiel.python.algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import contextlib\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from open_spiel.python.algorithms.deep_cfr import AdvantageMemory, StrategyMemory, ReservoirBuffer\n",
    "\n",
    "from open_spiel.python import policy\n",
    "import pyspiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the shuffle buffer used to reshuffle part of the data each\n",
    "# epoch within one training iteration\n",
    "ADVANTAGE_TRAIN_SHUFFLE_SIZE = 100000\n",
    "STRATEGY_TRAIN_SHUFFLE_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipDense(tf.keras.layers.Layer):\n",
    "  \"\"\"Dense Layer with skip connection.\"\"\"\n",
    "\n",
    "  def __init__(self, units, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.hidden = tf.keras.layers.Dense(units, kernel_initializer='he_normal')\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.hidden(x) + x\n",
    "\n",
    "\n",
    "class PolicyNetwork(tf.keras.Model):\n",
    "  \"\"\"Implements the policy network as an MLP.\n",
    "\n",
    "  Implements the policy network as a MLP with skip connections in adjacent\n",
    "  layers with the same number of units, except for the last hidden connection\n",
    "  where a layer normalization is applied.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_size,\n",
    "               policy_network_layers,\n",
    "               num_actions,\n",
    "               activation='leakyrelu',\n",
    "               **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self._input_size = input_size\n",
    "    self._num_actions = num_actions\n",
    "    if activation == 'leakyrelu':\n",
    "      self.activation = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "    elif activation == 'relu':\n",
    "      self.activation = tf.keras.layers.ReLU()\n",
    "    else:\n",
    "      self.activation = activation\n",
    "\n",
    "    self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    self.hidden = []\n",
    "    prevunits = 0\n",
    "    for units in policy_network_layers[:-1]:\n",
    "      if prevunits == units:\n",
    "        self.hidden.append(SkipDense(units))\n",
    "      else:\n",
    "        self.hidden.append(\n",
    "            tf.keras.layers.Dense(units, kernel_initializer='he_normal'))\n",
    "      prevunits = units\n",
    "    self.normalization = tf.keras.layers.LayerNormalization()\n",
    "    self.lastlayer = tf.keras.layers.Dense(\n",
    "        policy_network_layers[-1], kernel_initializer='he_normal')\n",
    "\n",
    "    self.out_layer = tf.keras.layers.Dense(num_actions)\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Applies Policy Network.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tuple representing (info_state, legal_action_mask)\n",
    "\n",
    "    Returns:\n",
    "        Action probabilities\n",
    "    \"\"\"\n",
    "    x, mask = inputs\n",
    "    for layer in self.hidden:\n",
    "      x = layer(x)\n",
    "      x = self.activation(x)\n",
    "\n",
    "    x = self.normalization(x)\n",
    "    x = self.lastlayer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.out_layer(x)\n",
    "    x = tf.where(mask == 1, x, -10e20)\n",
    "    x = self.softmax(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class AdvantageNetwork(tf.keras.Model):\n",
    "  \"\"\"Implements the advantage network as an MLP.\n",
    "\n",
    "  Implements the advantage network as an MLP with skip connections in\n",
    "  adjacent layers with the same number of units, except for the last hidden\n",
    "  connection where a layer normalization is applied.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               input_size,\n",
    "               adv_network_layers,\n",
    "               num_actions,\n",
    "               activation='leakyrelu',\n",
    "               **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self._input_size = input_size\n",
    "    self._num_actions = num_actions\n",
    "    if activation == 'leakyrelu':\n",
    "      self.activation = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "    elif activation == 'relu':\n",
    "      self.activation = tf.keras.layers.ReLU()\n",
    "    else:\n",
    "      self.activation = activation\n",
    "\n",
    "    self.hidden = []\n",
    "    prevunits = 0\n",
    "    for units in adv_network_layers[:-1]:\n",
    "      if prevunits == units:\n",
    "        self.hidden.append(SkipDense(units))\n",
    "      else:\n",
    "        self.hidden.append(\n",
    "            tf.keras.layers.Dense(units, kernel_initializer='he_normal'))\n",
    "      prevunits = units\n",
    "    self.normalization = tf.keras.layers.LayerNormalization()\n",
    "    self.lastlayer = tf.keras.layers.Dense(\n",
    "        adv_network_layers[-1], kernel_initializer='he_normal')\n",
    "\n",
    "    self.out_layer = tf.keras.layers.Dense(num_actions)\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Applies Policy Network.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tuple representing (info_state, legal_action_mask)\n",
    "\n",
    "    Returns:\n",
    "        Cumulative regret for each info_state action\n",
    "    \"\"\"\n",
    "    x, mask = inputs\n",
    "    for layer in self.hidden:\n",
    "      x = layer(x)\n",
    "      x = self.activation(x)\n",
    "\n",
    "    x = self.normalization(x)\n",
    "    x = self.lastlayer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.out_layer(x)\n",
    "    x = mask * x\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCFRSolver(policy.Policy):\n",
    "  \"\"\"Implements a solver for the Deep CFR Algorithm.\n",
    "\n",
    "  See https://arxiv.org/abs/1811.00164.\n",
    "\n",
    "  Define all networks and sampling buffers/memories.  Derive losses & learning\n",
    "  steps. Initialize the game state and algorithmic variables.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               game,\n",
    "               policy_network_layers=(256, 256),\n",
    "               advantage_network_layers=(128, 128),\n",
    "               num_iterations: int = 100,\n",
    "               num_traversals: int = 100,\n",
    "               learning_rate: float = 1e-3,\n",
    "               batch_size_advantage: int = 2048,\n",
    "               batch_size_strategy: int = 2048,\n",
    "               memory_capacity: int = int(1e6),\n",
    "               policy_network_train_steps: int = 5000,\n",
    "               advantage_network_train_steps: int = 750,\n",
    "               reinitialize_advantage_networks: bool = True,\n",
    "               save_advantage_networks: str = None,\n",
    "               save_strategy_memories: str = None,\n",
    "               infer_device='cpu',\n",
    "               train_device='cpu'):\n",
    "    \"\"\"Initialize the Deep CFR algorithm.\n",
    "\n",
    "    Args:\n",
    "      game: Open Spiel game.\n",
    "      policy_network_layers: (list[int]) Layer sizes of strategy net MLP.\n",
    "      advantage_network_layers: (list[int]) Layer sizes of advantage net MLP.\n",
    "      num_iterations: Number of iterations.\n",
    "      num_traversals: Number of traversals per iteration.\n",
    "      learning_rate: Learning rate.\n",
    "      batch_size_advantage: (int) Batch size to sample from advantage memories.\n",
    "      batch_size_strategy: (int) Batch size to sample from strategy memories.\n",
    "      memory_capacity: Number of samples that can be stored in memory.\n",
    "      policy_network_train_steps: Number of policy network training steps (one\n",
    "        policy training iteration at the end).\n",
    "      advantage_network_train_steps: Number of advantage network training steps\n",
    "        (per iteration).\n",
    "      reinitialize_advantage_networks: Whether to re-initialize the advantage\n",
    "        network before training on each iteration.\n",
    "      save_advantage_networks: If provided, all advantage network itearations\n",
    "        are saved in the given folder. This can be useful to implement SD-CFR\n",
    "        https://arxiv.org/abs/1901.07621\n",
    "      save_strategy_memories: saves the collected strategy memories as a\n",
    "        tfrecords file in the given location. This is not affected by\n",
    "        memory_capacity. All memories are saved to disk and not kept in memory\n",
    "      infer_device: device used for TF-operations in the traversal branch.\n",
    "        Format is anything accepted by tf.device\n",
    "      train_device: device used for TF-operations in the NN training steps.\n",
    "        Format is anything accepted by tf.device\n",
    "    \"\"\"\n",
    "    all_players = list(range(game.num_players()))\n",
    "    super(DeepCFRSolver, self).__init__(game, all_players)\n",
    "    self._game = game\n",
    "    if game.get_type().dynamics == pyspiel.GameType.Dynamics.SIMULTANEOUS:\n",
    "      # `_traverse_game_tree` does not take into account this option.\n",
    "      raise ValueError('Simulatenous games are not supported.')\n",
    "    self._batch_size_advantage = batch_size_advantage\n",
    "    self._batch_size_strategy = batch_size_strategy\n",
    "    self._policy_network_train_steps = policy_network_train_steps\n",
    "    self._advantage_network_train_steps = advantage_network_train_steps\n",
    "    self._policy_network_layers = policy_network_layers\n",
    "    self._advantage_network_layers = advantage_network_layers\n",
    "    self._num_players = game.num_players()\n",
    "    self._root_node = self._game.new_initial_state()\n",
    "    self._embedding_size = len(self._root_node.information_state_tensor(0))\n",
    "    self._num_iterations = num_iterations\n",
    "    self._num_traversals = num_traversals\n",
    "    self._reinitialize_advantage_networks = reinitialize_advantage_networks\n",
    "    self._num_actions = game.num_distinct_actions()\n",
    "    self._iteration = 1\n",
    "    self._learning_rate = learning_rate\n",
    "    self._save_advantage_networks = save_advantage_networks\n",
    "    self._save_strategy_memories = save_strategy_memories\n",
    "    self._infer_device = infer_device\n",
    "    self._train_device = train_device\n",
    "    self._memories_tfrecordpath = None\n",
    "    self._memories_tfrecordfile = None\n",
    "\n",
    "    # Initialize file save locations\n",
    "    if self._save_advantage_networks:\n",
    "      os.makedirs(self._save_advantage_networks, exist_ok=True)\n",
    "\n",
    "    if self._save_strategy_memories:\n",
    "      if os.path.isdir(self._save_strategy_memories):\n",
    "        self._memories_tfrecordpath = os.path.join(\n",
    "            self._save_strategy_memories, 'strategy_memories.tfrecord')\n",
    "      else:\n",
    "        os.makedirs(\n",
    "            os.path.split(self._save_strategy_memories)[0], exist_ok=True)\n",
    "        self._memories_tfrecordpath = self._save_strategy_memories\n",
    "\n",
    "    # Initialize policy network, loss, optmizer\n",
    "    self._reinitialize_policy_network()\n",
    "\n",
    "    # Initialize advantage networks, losses, optmizers\n",
    "    self._adv_networks = []\n",
    "    self._adv_networks_train = []\n",
    "    self._loss_advantages = []\n",
    "    self._optimizer_advantages = []\n",
    "    self._advantage_train_step = []\n",
    "    for player in range(self._num_players):\n",
    "      self._adv_networks.append(\n",
    "          AdvantageNetwork(self._embedding_size, self._advantage_network_layers,\n",
    "                           self._num_actions))\n",
    "      with tf.device(self._train_device):\n",
    "        self._adv_networks_train.append(\n",
    "            AdvantageNetwork(self._embedding_size,\n",
    "                             self._advantage_network_layers, self._num_actions))\n",
    "        self._loss_advantages.append(tf.keras.losses.MeanSquaredError())\n",
    "        self._optimizer_advantages.append(\n",
    "            tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "        self._advantage_train_step.append(\n",
    "            self._get_advantage_train_graph(player))\n",
    "\n",
    "    self._create_memories(memory_capacity)\n",
    "\n",
    "  def _reinitialize_policy_network(self):\n",
    "    \"\"\"Reinitalize policy network and optimizer for training.\"\"\"\n",
    "    with tf.device(self._train_device):\n",
    "      self._policy_network = PolicyNetwork(self._embedding_size,\n",
    "                                           self._policy_network_layers,\n",
    "                                           self._num_actions)\n",
    "      self._optimizer_policy = tf.keras.optimizers.Adam(\n",
    "          learning_rate=self._learning_rate)\n",
    "      self._loss_policy = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "  def _reinitialize_advantage_network(self, player):\n",
    "    \"\"\"Reinitalize player's advantage network and optimizer for training.\"\"\"\n",
    "    with tf.device(self._train_device):\n",
    "      self._adv_networks_train[player] = AdvantageNetwork(\n",
    "          self._embedding_size, self._advantage_network_layers,\n",
    "          self._num_actions)\n",
    "      self._optimizer_advantages[player] = tf.keras.optimizers.Adam(\n",
    "          learning_rate=self._learning_rate)\n",
    "      self._advantage_train_step[player] = (\n",
    "          self._get_advantage_train_graph(player))\n",
    "\n",
    "  @property\n",
    "  def advantage_buffers(self):\n",
    "    return self._advantage_memories\n",
    "\n",
    "  @property\n",
    "  def strategy_buffer(self):\n",
    "    return self._strategy_memories\n",
    "\n",
    "  def clear_advantage_buffers(self):\n",
    "    for p in range(self._num_players):\n",
    "      self._advantage_memories[p].clear()\n",
    "\n",
    "  def _create_memories(self, memory_capacity):\n",
    "    \"\"\"Create memory buffers and associated feature descriptions.\"\"\"\n",
    "    self._strategy_memories = ReservoirBuffer(memory_capacity)\n",
    "    self._advantage_memories = [\n",
    "        ReservoirBuffer(memory_capacity) for _ in range(self._num_players)\n",
    "    ]\n",
    "    self._strategy_feature_description = {\n",
    "        'info_state': tf.io.FixedLenFeature([self._embedding_size], tf.float32),\n",
    "        'action_probs': tf.io.FixedLenFeature([self._num_actions], tf.float32),\n",
    "        'iteration': tf.io.FixedLenFeature([1], tf.float32),\n",
    "        'legal_actions': tf.io.FixedLenFeature([self._num_actions], tf.float32)\n",
    "    }\n",
    "    self._advantage_feature_description = {\n",
    "        'info_state': tf.io.FixedLenFeature([self._embedding_size], tf.float32),\n",
    "        'iteration': tf.io.FixedLenFeature([1], tf.float32),\n",
    "        'samp_regret': tf.io.FixedLenFeature([self._num_actions], tf.float32),\n",
    "        'legal_actions': tf.io.FixedLenFeature([self._num_actions], tf.float32)\n",
    "    }\n",
    "\n",
    "  def solve(self):\n",
    "    \"\"\"Solution logic for Deep CFR.\"\"\"\n",
    "    advantage_losses = collections.defaultdict(list)\n",
    "    with tf.device(self._infer_device):\n",
    "      with contextlib.ExitStack() as stack:\n",
    "        if self._save_strategy_memories:\n",
    "          self._memories_tfrecordfile = stack.enter_context(\n",
    "              tf.io.TFRecordWriter(self._memories_tfrecordpath))\n",
    "        for _ in range(self._num_iterations):\n",
    "          for p in range(self._num_players):\n",
    "            for _ in range(self._num_traversals):\n",
    "              self._traverse_game_tree(self._root_node, p)\n",
    "            if self._reinitialize_advantage_networks:\n",
    "              # Re-initialize advantage network for p and train from scratch.\n",
    "              self._reinitialize_advantage_network(p)\n",
    "            advantage_losses[p].append(self._learn_advantage_network(p))\n",
    "            if self._save_advantage_networks:\n",
    "              os.makedirs(self._save_advantage_networks, exist_ok=True)\n",
    "              self._adv_networks[p].save(\n",
    "                  os.path.join(self._save_advantage_networks,\n",
    "                               f'advnet_p{p}_it{self._iteration:04}'))\n",
    "          self._iteration += 1\n",
    "    # Train policy network.\n",
    "    policy_loss = self._learn_strategy_network()\n",
    "    return self._policy_network, advantage_losses, policy_loss\n",
    "\n",
    "  def save_policy_network(self, outputfolder):\n",
    "    \"\"\"Saves the policy network to the given folder.\"\"\"\n",
    "    os.makedirs(outputfolder, exist_ok=True)\n",
    "    self._policy_network.save(outputfolder)\n",
    "\n",
    "  def train_policy_network_from_file(self,\n",
    "                                     tfrecordpath,\n",
    "                                     iteration=None,\n",
    "                                     batch_size_strategy=None,\n",
    "                                     policy_network_train_steps=None,\n",
    "                                     reinitialize_policy_network=True):\n",
    "    \"\"\"Trains the policy network from a previously stored tfrecords-file.\"\"\"\n",
    "    self._memories_tfrecordpath = tfrecordpath\n",
    "    if iteration:\n",
    "      self._iteration = iteration\n",
    "    if batch_size_strategy:\n",
    "      self._batch_size_strategy = batch_size_strategy\n",
    "    if policy_network_train_steps:\n",
    "      self._policy_network_train_steps = policy_network_train_steps\n",
    "    if reinitialize_policy_network:\n",
    "      self._reinitialize_policy_network()\n",
    "    policy_loss = self._learn_strategy_network()\n",
    "    return policy_loss\n",
    "\n",
    "  def _add_to_strategy_memory(self, info_state, iteration,\n",
    "                              strategy_action_probs, legal_actions_mask):\n",
    "    # pylint: disable=g-doc-args\n",
    "    \"\"\"Adds the given strategy data to the memory.\n",
    "\n",
    "    Uses either a tfrecordsfile on disk if provided, or a reservoir buffer.\n",
    "    \"\"\"\n",
    "    serialized_example = self._serialize_strategy_memory(\n",
    "        info_state, iteration, strategy_action_probs, legal_actions_mask)\n",
    "    if self._save_strategy_memories:\n",
    "      self._memories_tfrecordfile.write(serialized_example)\n",
    "    else:\n",
    "      self._strategy_memories.add(serialized_example)\n",
    "\n",
    "  def _serialize_strategy_memory(self, info_state, iteration,\n",
    "                                 strategy_action_probs, legal_actions_mask):\n",
    "    \"\"\"Create serialized example to store a strategy entry.\"\"\"\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'info_state':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=info_state)),\n",
    "                'action_probs':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(\n",
    "                            value=strategy_action_probs)),\n",
    "                'iteration':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=[iteration])),\n",
    "                'legal_actions':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=legal_actions_mask))\n",
    "            }))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "  def _deserialize_strategy_memory(self, serialized):\n",
    "    \"\"\"Deserializes a batch of strategy examples for the train step.\"\"\"\n",
    "    tups = tf.io.parse_example(serialized, self._strategy_feature_description)\n",
    "    return (tups['info_state'], tups['action_probs'], tups['iteration'],\n",
    "            tups['legal_actions'])\n",
    "\n",
    "  def _serialize_advantage_memory(self, info_state, iteration, samp_regret,\n",
    "                                  legal_actions_mask):\n",
    "    \"\"\"Create serialized example to store an advantage entry.\"\"\"\n",
    "    example = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                'info_state':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=info_state)),\n",
    "                'iteration':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=[iteration])),\n",
    "                'samp_regret':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=samp_regret)),\n",
    "                'legal_actions':\n",
    "                    tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=legal_actions_mask))\n",
    "            }))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "  def _deserialize_advantage_memory(self, serialized):\n",
    "    \"\"\"Deserializes a batch of advantage examples for the train step.\"\"\"\n",
    "    tups = tf.io.parse_example(serialized, self._advantage_feature_description)\n",
    "    return (tups['info_state'], tups['samp_regret'], tups['iteration'],\n",
    "            tups['legal_actions'])\n",
    "\n",
    "  def _traverse_game_tree(self, state, player):\n",
    "    \"\"\"Performs a traversal of the game tree using external sampling.\n",
    "\n",
    "    Over a traversal the advantage and strategy memories are populated with\n",
    "    computed advantage values and matched regrets respectively.\n",
    "\n",
    "    Args:\n",
    "      state: Current OpenSpiel game state.\n",
    "      player: (int) Player index for this traversal.\n",
    "\n",
    "    Returns:\n",
    "      Recursively returns expected payoffs for each action.\n",
    "    \"\"\"\n",
    "    if state.is_terminal():\n",
    "      # Terminal state get returns.\n",
    "      return state.returns()[player]\n",
    "    elif state.is_chance_node():\n",
    "      # If this is a chance node, sample an action\n",
    "      action = np.random.choice([i[0] for i in state.chance_outcomes()])\n",
    "      return self._traverse_game_tree(state.child(action), player)\n",
    "    elif state.current_player() == player:\n",
    "      # Update the policy over the info set & actions via regret matching.\n",
    "      _, strategy = self._sample_action_from_advantage(state, player)\n",
    "      exp_payoff = 0 * strategy\n",
    "      for action in state.legal_actions():\n",
    "        exp_payoff[action] = self._traverse_game_tree(\n",
    "            state.child(action), player)\n",
    "      ev = np.sum(exp_payoff * strategy)\n",
    "      samp_regret = (exp_payoff - ev) * state.legal_actions_mask(player)\n",
    "      self._advantage_memories[player].add(\n",
    "          self._serialize_advantage_memory(state.information_state_tensor(),\n",
    "                                           self._iteration, samp_regret,\n",
    "                                           state.legal_actions_mask(player)))\n",
    "      return ev\n",
    "    else:\n",
    "      other_player = state.current_player()\n",
    "      _, strategy = self._sample_action_from_advantage(state, other_player)\n",
    "      # Recompute distribution for numerical errors.\n",
    "      probs = strategy\n",
    "      probs /= probs.sum()\n",
    "      sampled_action = np.random.choice(range(self._num_actions), p=probs)\n",
    "      self._add_to_strategy_memory(\n",
    "          state.information_state_tensor(other_player), self._iteration,\n",
    "          strategy, state.legal_actions_mask(other_player))\n",
    "      return self._traverse_game_tree(state.child(sampled_action), player)\n",
    "\n",
    "  @tf.function\n",
    "  def _get_matched_regrets(self, info_state, legal_actions_mask, player):\n",
    "    \"\"\"TF-Graph to calculate regret matching.\"\"\"\n",
    "    advs = self._adv_networks[player](\n",
    "        (tf.expand_dims(info_state, axis=0), legal_actions_mask),\n",
    "        training=False)[0]\n",
    "    advantages = tf.maximum(advs, 0)\n",
    "    summed_regret = tf.reduce_sum(advantages)\n",
    "    if summed_regret > 0:\n",
    "      matched_regrets = advantages / summed_regret\n",
    "    else:\n",
    "      matched_regrets = tf.one_hot(\n",
    "          tf.argmax(tf.where(legal_actions_mask == 1, advs, -10e20)),\n",
    "          self._num_actions)\n",
    "    return advantages, matched_regrets\n",
    "\n",
    "  def _sample_action_from_advantage(self, state, player):\n",
    "    \"\"\"Returns an info state policy by applying regret-matching.\n",
    "\n",
    "    Args:\n",
    "      state: Current OpenSpiel game state.\n",
    "      player: (int) Player index over which to compute regrets.\n",
    "\n",
    "    Returns:\n",
    "      1. (np-array) Advantage values for info state actions indexed by action.\n",
    "      2. (np-array) Matched regrets, prob for actions indexed by action.\n",
    "    \"\"\"\n",
    "    info_state = tf.constant(\n",
    "        state.information_state_tensor(player), dtype=tf.float32)\n",
    "    legal_actions_mask = tf.constant(\n",
    "        state.legal_actions_mask(player), dtype=tf.float32)\n",
    "    advantages, matched_regrets = self._get_matched_regrets(\n",
    "        info_state, legal_actions_mask, player)\n",
    "    return advantages.numpy(), matched_regrets.numpy()\n",
    "\n",
    "  def action_probabilities(self, state):\n",
    "    \"\"\"Returns action probabilities dict for a single batch.\"\"\"\n",
    "    cur_player = state.current_player()\n",
    "    legal_actions = state.legal_actions(cur_player)\n",
    "    legal_actions_mask = tf.constant(\n",
    "        state.legal_actions_mask(cur_player), dtype=tf.float32)\n",
    "    info_state_vector = tf.constant(\n",
    "        state.information_state_tensor(), dtype=tf.float32)\n",
    "    if len(info_state_vector.shape) == 1:\n",
    "      info_state_vector = tf.expand_dims(info_state_vector, axis=0)\n",
    "    probs = self._policy_network((info_state_vector, legal_actions_mask),\n",
    "                                 training=False)\n",
    "    probs = probs.numpy()\n",
    "    return {action: probs[0][action] for action in legal_actions}\n",
    "\n",
    "  def _get_advantage_dataset(self, player):\n",
    "    \"\"\"Returns the collected regrets for the given player as a dataset.\"\"\"\n",
    "    self._advantage_memories[player].shuffle_data()\n",
    "    data = tf.data.Dataset.from_tensor_slices(\n",
    "        self._advantage_memories[player].data)\n",
    "    data = data.shuffle(ADVANTAGE_TRAIN_SHUFFLE_SIZE)\n",
    "    data = data.repeat()\n",
    "    data = data.batch(self._batch_size_advantage)\n",
    "    data = data.map(self._deserialize_advantage_memory)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data\n",
    "\n",
    "  def _get_advantage_train_graph(self, player):\n",
    "    \"\"\"Return TF-Graph to perform advantage network train step.\"\"\"\n",
    "    @tf.function\n",
    "    def train_step(info_states, advantages, iterations, masks, iteration):\n",
    "      model = self._adv_networks_train[player]\n",
    "      with tf.GradientTape() as tape:\n",
    "        preds = model((info_states, masks), training=True)\n",
    "        main_loss = self._loss_advantages[player](\n",
    "            advantages, preds, sample_weight=iterations * 2 / iteration)\n",
    "        loss = tf.add_n([main_loss], model.losses)\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "      self._optimizer_advantages[player].apply_gradients(\n",
    "          zip(gradients, model.trainable_variables))\n",
    "      return main_loss\n",
    "\n",
    "    return train_step\n",
    "\n",
    "  def _learn_advantage_network(self, player):\n",
    "    \"\"\"Compute the loss on sampled transitions and perform a Q-network update.\n",
    "\n",
    "    If there are not enough elements in the buffer, no loss is computed and\n",
    "    `None` is returned instead.\n",
    "\n",
    "    Args:\n",
    "      player: (int) player index.\n",
    "\n",
    "    Returns:\n",
    "      The average loss over the advantage network of the last batch.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device(self._train_device):\n",
    "      tfit = tf.constant(self._iteration, dtype=tf.float32)\n",
    "      data = self._get_advantage_dataset(player)\n",
    "      for d in data.take(self._advantage_network_train_steps):\n",
    "        main_loss = self._advantage_train_step[player](*d, tfit)\n",
    "\n",
    "    self._adv_networks[player].set_weights(\n",
    "        self._adv_networks_train[player].get_weights())\n",
    "    return main_loss\n",
    "\n",
    "  def _get_strategy_dataset(self):\n",
    "    \"\"\"Returns the collected strategy memories as a dataset.\"\"\"\n",
    "    if self._memories_tfrecordpath:\n",
    "      data = tf.data.TFRecordDataset(self._memories_tfrecordpath)\n",
    "    else:\n",
    "      self._strategy_memories.shuffle_data()\n",
    "      data = tf.data.Dataset.from_tensor_slices(self._strategy_memories.data)\n",
    "    data = data.shuffle(STRATEGY_TRAIN_SHUFFLE_SIZE)\n",
    "    data = data.repeat()\n",
    "    data = data.batch(self._batch_size_strategy)\n",
    "    data = data.map(self._deserialize_strategy_memory)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data\n",
    "\n",
    "  def _learn_strategy_network(self):\n",
    "    \"\"\"Compute the loss over the strategy network.\n",
    "\n",
    "    Returns:\n",
    "      The average loss obtained on the last training batch of transitions\n",
    "      or `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(info_states, action_probs, iterations, masks):\n",
    "      model = self._policy_network\n",
    "      with tf.GradientTape() as tape:\n",
    "        preds = model((info_states, masks), training=True)\n",
    "        main_loss = self._loss_policy(\n",
    "            action_probs, preds, sample_weight=iterations * 2 / self._iteration)\n",
    "        loss = tf.add_n([main_loss], model.losses)\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "      self._optimizer_policy.apply_gradients(\n",
    "          zip(gradients, model.trainable_variables))\n",
    "      return main_loss\n",
    "\n",
    "    with tf.device(self._train_device):\n",
    "      data = self._get_strategy_dataset()\n",
    "      for d in data.take(self._policy_network_train_steps):\n",
    "        main_loss = train_step(*d)\n",
    "\n",
    "    return main_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game('kuhn_poker', {\"players\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DeepCFRSolver(game,\n",
    "      policy_network_layers=(32, 32),\n",
    "      advantage_network_layers=(16, 16),\n",
    "      num_iterations=400,\n",
    "      num_traversals=40,\n",
    "      learning_rate=1e-3,\n",
    "      batch_size_advantage=None,\n",
    "      batch_size_strategy=None,\n",
    "      memory_capacity=int(1e6),\n",
    "      infer_device='cpu',\n",
    "      train_device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game,\n",
    "# policy_network_layers=(256, 256),\n",
    "# advantage_network_layers=(128, 128),\n",
    "# num_iterations: int = 100,\n",
    "# num_traversals: int = 100,\n",
    "# learning_rate: float = 1e-3,\n",
    "# batch_size_advantage: int = 2048,\n",
    "# batch_size_strategy: int = 2048,\n",
    "# memory_capacity: int = int(1e6),\n",
    "# policy_network_train_steps: int = 5000,\n",
    "# advantage_network_train_steps: int = 750,\n",
    "# reinitialize_advantage_networks: bool = True,\n",
    "# save_advantage_networks: str = None,\n",
    "# save_strategy_memories: str = None,\n",
    "# infer_device='cpu',\n",
    "# train_device='cpu'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to capture an EagerTensor without building a function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-590a6083cc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-e19f28e7b0f2>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_players\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_traversals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traverse_game_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reinitialize_advantage_networks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m               \u001b[0;31m# Re-initialize advantage network for p and train from scratch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e19f28e7b0f2>\u001b[0m in \u001b[0;36m_traverse_game_tree\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    310\u001b[0m       \u001b[0;31m# If this is a chance node, sample an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchance_outcomes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traverse_game_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0;31m# Update the policy over the info set & actions via regret matching.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e19f28e7b0f2>\u001b[0m in \u001b[0;36m_traverse_game_tree\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    310\u001b[0m       \u001b[0;31m# If this is a chance node, sample an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchance_outcomes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traverse_game_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0;31m# Update the policy over the info set & actions via regret matching.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e19f28e7b0f2>\u001b[0m in \u001b[0;36m_traverse_game_tree\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0;31m# Update the policy over the info set & actions via regret matching.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_action_from_advantage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m       \u001b[0mexp_payoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e19f28e7b0f2>\u001b[0m in \u001b[0;36m_sample_action_from_advantage\u001b[0;34m(self, state, player)\u001b[0m\n\u001b[1;32m    369\u001b[0m         state.legal_actions_mask(player), dtype=tf.float32)\n\u001b[1;32m    370\u001b[0m     advantages, matched_regrets = self._get_matched_regrets(\n\u001b[0;32m--> 371\u001b[0;31m         info_state, legal_actions_mask, player)\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madvantages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_regrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 957\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1975\u001b[0m           {\"PartitionedCall\": self._get_gradient_function(),\n\u001b[1;32m   1976\u001b[0m            \"StatefulPartitionedCall\": self._get_gradient_function()}):\n\u001b[0;32m-> 1977\u001b[0;31m         \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_tangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1978\u001b[0m     \u001b[0mforward_backward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 executor_type=executor_type)\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36mpartitioned_call\u001b[0;34m(args, f, tout, executing_eagerly, config, executor_type)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;31m# The generated binding returns an empty list for functions that don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m   \u001b[0;31m# return any Tensors, hence the need to use `create_op` directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m   tin_attr = attr_value_pb2.AttrValue(\n\u001b[1;32m   1191\u001b[0m       list=attr_value_pb2.AttrValue.ListValue(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/functional_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;31m# The generated binding returns an empty list for functions that don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m   \u001b[0;31m# return any Tensors, hence the need to use `create_op` directly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m   tin_attr = attr_value_pb2.AttrValue(\n\u001b[1;32m   1191\u001b[0m       list=attr_value_pb2.AttrValue.ListValue(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1523\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m         raise RuntimeError(\"Attempting to capture an EagerTensor without \"\n\u001b[0m\u001b[1;32m   1526\u001b[0m                            \"building a function.\")\n\u001b[1;32m   1527\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to capture an EagerTensor without building a function."
     ]
    }
   ],
   "source": [
    "_, advantage_losses, policy_loss = solver.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

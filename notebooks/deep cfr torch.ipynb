{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from open_spiel.python import policy\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "from open_spiel.python.algorithms.deep_cfr import AdvantageMemory, StrategyMemory, ReservoirBuffer\n",
    "import pyspiel\n",
    "from absl import logging\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_params(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    m.reset_parameters()\n",
    "\n",
    "# Defining fully-connected network to use as policy and advantage network\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, layers):\n",
    "    \"\"\"\n",
    "      Creates a MLP with hidden units defined by `layers`.\n",
    "    \"\"\"\n",
    "    super(MLP, self).__init__()\n",
    "    layers = [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers[:-1]))]\n",
    "    self._model = nn.Sequential(*layers)\n",
    "    \n",
    "  def forward(self, input):\n",
    "    return self._model(input)\n",
    "  \n",
    "  def reset(self):\n",
    "    self._model.apply(reset_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepCFR(policy.Policy):\n",
    "  \"\"\"Implements a solver for the Deep CFR Algorithm with PyTorch.\n",
    "\n",
    "  See https://arxiv.org/abs/1811.00164.\n",
    "\n",
    "  Define all networks and sampling buffers/memories.  Derive losses & learning\n",
    "  steps. Initialize the game state and algorithmic variables.\n",
    "\n",
    "  Note: batch sizes default to `None` implying that training over the full\n",
    "        dataset in memory is done by default.  To sample from the memories you\n",
    "        may set these values to something less than the full capacity of the\n",
    "        memory.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               game,\n",
    "               policy_network_layers=(256, 256),\n",
    "               advantage_network_layers=(128, 128),\n",
    "               num_iterations: int = 100,\n",
    "               num_traversals: int = 20,\n",
    "               learning_rate:float = 1e-4,\n",
    "               batch_size_advantage=None,\n",
    "               batch_size_strategy=None,\n",
    "               memory_capacity: int =int(1e6),\n",
    "               policy_network_train_steps: int = 1,\n",
    "               advantage_network_train_steps: int = 1,\n",
    "               reinitialize_advantage_networks: bool = True):\n",
    "    \"\"\"Initialize the Deep CFR algorithm.\n",
    "\n",
    "    Args:\n",
    "      game: Open Spiel game.\n",
    "      policy_network_layers: (list[int]) Layer sizes of strategy net MLP.\n",
    "      advantage_network_layers: (list[int]) Layer sizes of advantage net MLP.\n",
    "      num_iterations: (int) Number of training iterations.\n",
    "      num_traversals: (int) Number of traversals per iteration.\n",
    "      learning_rate: (float) Learning rate.\n",
    "      batch_size_advantage: (int or None) Batch size to sample from advantage\n",
    "        memories.\n",
    "      batch_size_strategy: (int or None) Batch size to sample from strategy\n",
    "        memories.\n",
    "      memory_capacity: Number af samples that can be stored in memory.\n",
    "      policy_network_train_steps: Number of policy network training steps (per\n",
    "        iteration).\n",
    "      advantage_network_train_steps: Number of advantage network training steps\n",
    "        (per iteration).\n",
    "      reinitialize_advantage_networks: Whether to re-initialize the\n",
    "        advantage network before training on each iteration.\n",
    "    \"\"\"\n",
    "    all_players = list(range(game.num_players()))\n",
    "    super(DeepCFR, self).__init__(game, all_players)\n",
    "    self._game = game\n",
    "    if game.get_type().dynamics == pyspiel.GameType.Dynamics.SIMULTANEOUS:\n",
    "      # `_traverse_game_tree` does not take into account this option.\n",
    "      raise ValueError(\"Simulatenous games are not supported.\")\n",
    "    self._batch_size_advantage = batch_size_advantage\n",
    "    self._batch_size_strategy = batch_size_strategy\n",
    "    self._policy_network_train_steps = policy_network_train_steps\n",
    "    self._advantage_network_train_steps = advantage_network_train_steps\n",
    "    self._num_players = game.num_players()\n",
    "    self._root_node = self._game.new_initial_state()\n",
    "    self._embedding_size = len(\n",
    "        self._root_node.information_state_tensor(0))\n",
    "    self._num_iterations = num_iterations\n",
    "    self._num_traversals = num_traversals\n",
    "    self._reinitialize_advantage_networks = reinitialize_advantage_networks\n",
    "    self._num_actions = game.num_distinct_actions()\n",
    "    self._iteration = 1\n",
    "\n",
    "    # Define strategy network, loss & memory.\n",
    "    self._strategy_memories = ReservoirBuffer(memory_capacity)\n",
    "    self._policy_network = MLP(\n",
    "      [self._embedding_size] + list(policy_network_layers) + [self._num_actions])\n",
    "    # Illegal actions are handled in the traversal code where expected payoff\n",
    "    # and sampled regret is computed from the advantage networks.\n",
    "    self._policy_sm = nn.Softmax(dim=-1)\n",
    "    self._loss_policy = nn.MSELoss()\n",
    "    self._optimizer_policy = torch.optim.Adam(\n",
    "                  self._policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Define advantage network, loss & memory. (One per player)\n",
    "    self._advantage_memories = [\n",
    "        ReservoirBuffer(memory_capacity) for _ in range(self._num_players)\n",
    "    ]\n",
    "    self._advantage_networks = [\n",
    "        MLP([self._embedding_size] + list(advantage_network_layers) + \n",
    "            [self._num_actions])\n",
    "        for _ in range(self._num_players)\n",
    "    ]\n",
    "    self._loss_advantages = nn.MSELoss(reduction='mean')\n",
    "    self._optimizer_advantages = []\n",
    "    for p in range(self._num_players):\n",
    "      self._optimizer_advantages.append(torch.optim.Adam(\n",
    "            self._advantage_networks[p].parameters(), lr=learning_rate))\n",
    "\n",
    "  @property\n",
    "  def advantage_buffers(self):\n",
    "    return self._advantage_memories\n",
    "\n",
    "  @property\n",
    "  def strategy_buffer(self):\n",
    "    return self._strategy_memories\n",
    "\n",
    "  def clear_advantage_buffers(self):\n",
    "    for p in range(self._num_players):\n",
    "      self._advantage_memories[p].clear()\n",
    "\n",
    "  def reinitialize_advantage_network(self, player):\n",
    "    self._advantage_networks[player].reset()\n",
    "\n",
    "  def reinitialize_advantage_networks(self):\n",
    "    for p in range(self._num_players):\n",
    "      self.reinitialize_advantage_network(p)\n",
    "\n",
    "  def solve(self):\n",
    "    \"\"\"Solution logic for Deep CFR.\n",
    "\n",
    "    Traverses the game tree, while storing the transitions for training\n",
    "    advantage and policy networks. \n",
    "\n",
    "    Returns:\n",
    "      1. (nn.Module) Instance of the trained policy network for inference. \n",
    "      2. (list of floats) Advantage network losses for \n",
    "        each player during each iteration.\n",
    "      3. (float) Policy loss.   \n",
    "    \"\"\"\n",
    "    advantage_losses = collections.defaultdict(list)\n",
    "    for _ in range(self._num_iterations):\n",
    "      for p in range(self._num_players):\n",
    "        for _ in range(self._num_traversals):\n",
    "          self._traverse_game_tree(self._root_node, p)\n",
    "        if self._reinitialize_advantage_networks:\n",
    "          # Re-initialize advantage network for player and train from scratch.\n",
    "          self.reinitialize_advantage_network(p)\n",
    "        # Re-initialize advantage networks and train from scratch.\n",
    "        advantage_losses[p].append(self._learn_advantage_network(p))\n",
    "      self._iteration += 1\n",
    "      # Train policy network.\n",
    "    policy_loss = self._learn_strategy_network()\n",
    "    return self._policy_network, advantage_losses, policy_loss\n",
    "\n",
    "  def _traverse_game_tree(self, state, player):\n",
    "    \"\"\"Performs a traversal of the game tree.\n",
    "\n",
    "    Over a traversal the advantage and strategy memories are populated with\n",
    "    computed advantage values and matched regrets respectively.\n",
    "\n",
    "    Args:\n",
    "      state: Current OpenSpiel game state.\n",
    "      player: (int) Player index for this traversal.\n",
    "\n",
    "    Returns:\n",
    "      (float) Recursively returns expected payoffs for each action.\n",
    "    \"\"\"\n",
    "    expected_payoff = collections.defaultdict(float)\n",
    "    if state.is_terminal():\n",
    "      # Terminal state get returns.\n",
    "      return state.returns()[player]\n",
    "    elif state.is_chance_node():\n",
    "      # If this is a chance node, sample an action\n",
    "      action = np.random.choice([i[0] for i in state.chance_outcomes()])\n",
    "      return self._traverse_game_tree(state.child(action), player)\n",
    "    elif state.current_player() == player:\n",
    "      sampled_regret = collections.defaultdict(float)\n",
    "      # Update the policy over the info set & actions via regret matching.\n",
    "      advantages, strategy = self._sample_action_from_advantage(state, player)\n",
    "      for action in state.legal_actions():\n",
    "        expected_payoff[action] = self._traverse_game_tree(\n",
    "            state.child(action), player)\n",
    "      for action in state.legal_actions():\n",
    "        sampled_regret[action] = expected_payoff[action]\n",
    "        for a_ in state.legal_actions():\n",
    "          sampled_regret[action] -= strategy[a_] * expected_payoff[a_]\n",
    "      sampled_regret_arr = [0] * self._num_actions\n",
    "      for action in sampled_regret:\n",
    "        sampled_regret_arr[action] = sampled_regret[action]\n",
    "      self._advantage_memories[player].add(\n",
    "          AdvantageMemory(state.information_state_tensor(),\n",
    "                          self._iteration, sampled_regret_arr, action))\n",
    "      return max(expected_payoff.values())\n",
    "    else:\n",
    "      other_player = state.current_player()\n",
    "      _, strategy = self._sample_action_from_advantage(state, other_player)\n",
    "      # Recompute distribution for numerical errors.\n",
    "      probs = np.array(strategy)\n",
    "      probs /= probs.sum()\n",
    "      sampled_action = np.random.choice(range(self._num_actions), p=probs)\n",
    "      self._strategy_memories.add(\n",
    "          StrategyMemory(\n",
    "              state.information_state_tensor(other_player),\n",
    "              self._iteration, strategy))\n",
    "      return self._traverse_game_tree(state.child(sampled_action), player)\n",
    "\n",
    "  def _sample_action_from_advantage(self, state, player):\n",
    "    \"\"\"Returns an info state policy by applying regret-matching.\n",
    "\n",
    "    Args:\n",
    "      state: Current OpenSpiel game state.\n",
    "      player: (int) Player index over which to compute regrets.\n",
    "\n",
    "    Returns:\n",
    "      1. (list) Advantage values for info state actions indexed by action.\n",
    "      2. (list) Matched regrets, prob for actions indexed by action.\n",
    "    \"\"\"\n",
    "    info_state = state.information_state_tensor(player)\n",
    "    legal_actions = state.legal_actions(player)\n",
    "    with torch.no_grad():\n",
    "      state_tensor = torch.FloatTensor(np.expand_dims(info_state, axis=0))\n",
    "      advantages = self._advantage_networks[player](state_tensor)[0].numpy()\n",
    "    advantages = [max(0., advantage) for advantage in advantages]\n",
    "    cumulative_regret = np.sum(\n",
    "                            [advantages[action] for action in legal_actions])\n",
    "    matched_regrets = np.array([0.] * self._num_actions)\n",
    "    for action in legal_actions:\n",
    "      if cumulative_regret > 0.:\n",
    "        matched_regrets[action] = advantages[action] / cumulative_regret\n",
    "      else:\n",
    "        matched_regrets[action] = 1 / self._num_actions\n",
    "    return advantages, matched_regrets\n",
    "\n",
    "  def action_probabilities(self, state):\n",
    "    \"\"\"Computes the action probabilities for the current player \n",
    "      in the given state.\n",
    "    \n",
    "    Args:\n",
    "      state: (pyspiel.State)\n",
    "    \n",
    "    Returns:\n",
    "      (dict) action probabilities for a single batch.\"\"\"\n",
    "    cur_player = state.current_player()\n",
    "    legal_actions = state.legal_actions(cur_player)\n",
    "    info_state_vector = np.array(state.information_state_tensor())\n",
    "    if len(info_state_vector.shape) == 1:\n",
    "      info_state_vector = np.expand_dims(info_state_vector, axis=0)\n",
    "    with torch.no_grad():\n",
    "      logits = self._policy_network(torch.FloatTensor(info_state_vector))\n",
    "      probs = self._policy_sm(logits).numpy()\n",
    "    return {action: probs[0][action] for action in legal_actions}\n",
    "\n",
    "  def _learn_advantage_network(self, player):\n",
    "    \"\"\"Compute the loss on sampled transitions and perform a Q-network update.\n",
    "\n",
    "    If there are not enough elements in the buffer, no loss is computed and\n",
    "    `None` is returned instead.\n",
    "\n",
    "    Args:\n",
    "      player: (int) player index.\n",
    "\n",
    "    Returns:\n",
    "      (float) The average loss over the advantage network.\n",
    "    \"\"\"\n",
    "    for _ in range(self._advantage_network_train_steps):\n",
    "\n",
    "      if self._batch_size_advantage:\n",
    "        if self._batch_size_advantage > len(self._advantage_memories[player]):\n",
    "          ## Skip if there aren't enough samples\n",
    "          return None\n",
    "        samples = self._advantage_memories[player].sample(\n",
    "            self._batch_size_advantage)\n",
    "      else:\n",
    "        samples = self._advantage_memories[player]\n",
    "      info_states = []\n",
    "      advantages = []\n",
    "      iterations = []\n",
    "      for s in samples:\n",
    "        info_states.append(s.info_state)\n",
    "        advantages.append(s.advantage)\n",
    "        iterations.append([s.iteration])\n",
    "      # Ensure some samples have been gathered.\n",
    "      if not info_states:\n",
    "        return None\n",
    "      self._optimizer_advantages[player].zero_grad()\n",
    "      advantages = torch.FloatTensor(np.array(advantages))\n",
    "      iters = torch.FloatTensor(np.sqrt(np.array(iterations)))\n",
    "      outputs = self._advantage_networks[player](\n",
    "                        torch.FloatTensor(np.array(info_states)))\n",
    "      loss_advantages = self._loss_advantages(iters * outputs, \n",
    "                                              iters * advantages)\n",
    "      loss_advantages.backward()\n",
    "      self._optimizer_advantages[player].step()\n",
    "\n",
    "    return loss_advantages.detach().numpy()\n",
    "\n",
    "  def _learn_strategy_network(self):\n",
    "    \"\"\"Compute the loss over the strategy network.\n",
    "\n",
    "    Returns:\n",
    "      (float) The average loss obtained on this batch of transitions or `None`.\n",
    "    \"\"\"\n",
    "    for _ in range(self._policy_network_train_steps):\n",
    "      if self._batch_size_strategy:\n",
    "        if self._batch_size_strategy > len(self._strategy_memories):\n",
    "          ## Skip if there aren't enough samples\n",
    "          return None\n",
    "        samples = self._strategy_memories.sample(self._batch_size_strategy)\n",
    "      else:\n",
    "        samples = self._strategy_memories\n",
    "      info_states = []\n",
    "      action_probs = []\n",
    "      iterations = []\n",
    "      for s in samples:\n",
    "        info_states.append(s.info_state)\n",
    "        action_probs.append(s.strategy_action_probs)\n",
    "        iterations.append([s.iteration])\n",
    "\n",
    "      self._optimizer_policy.zero_grad()\n",
    "      iters = torch.FloatTensor(np.sqrt(np.array(iterations)))\n",
    "      ac_probs = torch.FloatTensor(np.array(np.squeeze(action_probs)))\n",
    "      logits = self._policy_network(torch.FloatTensor(np.array(info_states)))\n",
    "      outputs = self._policy_sm(logits)\n",
    "      loss_strategy = self._loss_policy(iters * outputs, iters * ac_probs)\n",
    "      loss_strategy.backward()\n",
    "      self._optimizer_policy.step()\n",
    "\n",
    "    return loss_strategy.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game('kuhn_poker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DeepCFR(game,\n",
    "      policy_network_layers=(64, 64),\n",
    "      advantage_network_layers=(32, 32),\n",
    "      num_iterations=4000,\n",
    "      num_traversals=40,\n",
    "      learning_rate=1e-3,\n",
    "      batch_size_advantage=None,\n",
    "      batch_size_strategy=None,\n",
    "      memory_capacity=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-590a6083cc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvantage_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2038601152b0>\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinitialize_advantage_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# Re-initialize advantage networks and train from scratch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0madvantage_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learn_advantage_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m       \u001b[0;31m# Train policy network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2038601152b0>\u001b[0m in \u001b[0;36m_learn_advantage_network\u001b[0;34m(self, player)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0miters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       outputs = self._advantage_networks[player](\n\u001b[0;32m--> 275\u001b[0;31m                         torch.FloatTensor(np.array(info_states)))\n\u001b[0m\u001b[1;32m    276\u001b[0m       loss_advantages = self._loss_advantages(iters * outputs, \n\u001b[1;32m    277\u001b[0m                                               iters * advantages)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, advantage_losses, policy_loss = solver.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advantage for player: 0 [array(198.40143, dtype=float32), array(240.91035, dtype=float32), '...', array(423.79248, dtype=float32), array(418.14813, dtype=float32)]\n",
      "Advantage Buffer Size for player 0 47807\n",
      "Advantage for player: 1 [array(258.57477, dtype=float32), array(255.71365, dtype=float32), '...', array(587.1759, dtype=float32), array(546.4579, dtype=float32)]\n",
      "Advantage Buffer Size for player 1 32000\n",
      "Strategy Buffer Size: 112274\n",
      "Final policy loss: 58.365112\n",
      "Deep CFR - NashConv: 0.9445120799599207\n"
     ]
    }
   ],
   "source": [
    "for player, losses in list(advantage_losses.items()):\n",
    "  print(\"Advantage for player:\", player,\n",
    "                losses[:2] + [\"...\"] + losses[-2:])\n",
    "  print(\"Advantage Buffer Size for player\", player,\n",
    "                len(solver.advantage_buffers[player]))\n",
    "print(\"Strategy Buffer Size:\",\n",
    "              len(solver.strategy_buffer))\n",
    "print(\"Final policy loss:\", policy_loss)\n",
    "conv = exploitability.nash_conv(\n",
    "    game,\n",
    "    policy.tabular_policy_from_callable(game, solver.action_probabilities))\n",
    "print(\"Deep CFR - NashConv:\", conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
